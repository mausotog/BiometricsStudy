%!TEX root = bioPrediction_main.tex
\section{Discussion}
\textbf{Implications for HCI design:}
\todo{fill this area:  increasing knowledge worker productivity and well-being but some depth in this direction would greatly improve the work. }


\vspace{0.05in}
\noindent
\textbf{Target Workspaces:}
Currently, the cost of biometric sensors and necessary infrastructure, such as automated light and sound systems for adjusting the environment, makes our approach most appropriate for high-value workspaces, such as control rooms, command centers, or dispatch offices. However, as standard office settings become more personalizable (e.g., via adjustable desks, lighting, and sound showers) and sensor costs decrease, our approach could be applied to any office environment, and thus could impact a large percentage of modern workers. As in modern cars, temperature and lighting could be regulated on a per-person basis, which would allow the environment to react to the person's current state and to maximize each person's preferences and productivity (e.g., preferences of men and women in temperature~\cite{Karjalainen07}). Further research would be needed to identify how to balance needs across a group of office workers and how to handle conflicting levels between different group members.

\vspace{0.05in}
\noindent
\textbf{Imbalanced Data:}
Study participants provided highly imbalanced data in their survey responses, with most participants only taking advantage of a subset of the Likert-scale values and the data points mostly being clustered around the middle of the scale, as can be seen in Table~\ref{responseDistribution}. While some of the imbalance is expected due to certain classes, such as 'not stressed', being more common in the workplace, this imbalance also provides challenges in the training and assessment of a machine learning classifier, as also found by others, e.g.~\cite{Exler16}. We addressed this for the training by oversampling in case of few data samples for the individual models and undersampling in case of a general model where more data was available. Especially in light of this imbalance in the data, the results we achieved with our models are encouraging. For the assessment of the classifiers' performance we addressed the imbalance by not just presenting accuracy, but also by focusing on prediction and recall to examine the classifier's performance in predicting the infrequent (yet more important) cases, such as when a user is struggling to stay awake and an intervention or warning might be needed most. 
% \textbf{Unbalanced Data:}
% As shown in Table~\ref{responseDistribution}, the typical participant provided highly unbalanced survey responses, with most responses exhibiting a central tendency. Many participants provided zero or one very high (or very low) value on the Likert scale. In spite of this, our results show that by using biometric sensors and leveraging a machine learning approach, we are able to predict stress, focus, and awakeness better than the baseline, with improvements up to 84.64\%. Taking into account that the model correctly predicts many cases that happen infrequently, this is especially promising. These infrequent cases, such as when a user is highly stressed or struggling to stay awake, are in fact the most important cases to detect, as they are when intervention is needed most.

\vspace{0.05in}
\noindent
\textbf{Ground Truth and Self-Reporting:} 
One of the key points for developing a good classifier for awakeness, focus and stress, is to collect a valid ground truth to be used as the output measure. When designing the study, we therefore spent an extensive amount of time on determining the exact questions to ask in the experience sampling, consulting experts in the area, and basing the questions and wording on previous research and studies. Yet, the reliability and validity of self-reports have been questioned in the past due to subjective biases, lack of care in reporting, and the highly individual nature of reporting aspects such as stress~\cite{Hernandez11,Hovsepian15}. In addition, in contexts such as the workplace, employees might be afraid to genuinely report levels of aspects, such as sleepiness. Hence, there is a chance that the collected experience samples do not adequately reflect the ground truth of the underlying variable under investigation. It could even be the case that certain biometrics might represent a more accurate ground truth of the studied phenomena than the self-reports. This suggests that a more confirmative study rather than an inquiry study could be a better approach, and we will explore such routes in future work.


\vspace{0.05in}
\noindent
\textbf{Corpus Size:}
In this study we collected data from 14 professionals over the course of eight weeks. By scholarly standards this constitutes a large dataset --- especially given that the participants were not students. However, in the context of machine learning this corpus is relatively small. Thus, the model's overall performance, while promising, is only an indication of the performance that a larger corpus could provide. Our results show that there is a positive correlation between data added to the training corpus and the performance level of the model. This upwards trend likely continues, but a larger dataset is needed to confirm or reject this hypothesis. 

% \vspace{0.05in}
% \noindent
% \textbf{Data Collection:} 
% One advantage of our study is that it was conducted on professionals in a real office setting (i.e., not on students). However, the disadvantage to working with professionals is that business deadlines and other pressing real world issues lead to missing survey responses. Additionally, busy professionals often fall into the trap of always selecting the same value for a given question that they have answered many times, which reduces the accuracy of the collected data. To minimize the effect of this inertia, we advise collecting data at typical low-productivity times, such as in the early afternoon or on Mondays~\cite{mark2014bored}.

\vspace{0.05in}
\noindent
\textbf{Per-User Model vs. Across-Users Model:}
Our results show that individually trained models outperform on average a more general model. Yet, given the increase in model performance with training data set size, we expect that with enough data, a generic model will perform reasonably close to the individually trained models. This would have significant practical implications, eliminating the need for survey-based feedback (which is only necessary for model training), and thereby completely automating measurement of human aspects. Our follow up work will focus on gathering more data toward this end.


% \vspace{0.05in}
% \noindent
% \textbf{Per-User Model vs. Across-Users Model:}\
% While we do see variations across participants, and thus our work shows that personal 
% models perform better than generic models, we expect that with enough data, a generic 
% model may perform equally well. This would have significant practical implications, eliminating the need for survey-based feedback (which is only necessary for model training), thereby completely automating measurement of human aspects. Our follow up work will focus on gathering more data toward this end and on evaluating the performance of a generic model.


